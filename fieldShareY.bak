#include <stdio.h>
#include <stdlib.h>
#include "mesh.h"
#include "mpi.h"
/*
void MPI_TransferF_XplusFilter(Domain *D)
{
    int i,rank,numberData;
    int myrank, nTasks; 
    float *behindF;
    MPI_Status status;         
   
    MPI_Comm_size(MPI_COMM_WORLD, &nTasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);            

    numberData=5;
    behindF=(float *)malloc(numberData*sizeof(float )); 
    
    //Transferring even ~ odd cores             
    behindF[0]=D->field[D->nx].E1;
    behindF[1]=D->field[D->nx].Pr;
    behindF[2]=D->field[D->nx].Pl;
    behindF[3]=D->field[D->nx].Sr;
    behindF[4]=D->field[D->nx].Sl;
        
    if(myrank%2==1)
    {
       MPI_Recv(behindF,numberData, MPI_FLOAT, D->beforeCore, D->beforeCore, MPI_COMM_WORLD,&status);  
       D->field[0].E1=behindF[0];
       D->field[0].Pr=behindF[1];
       D->field[0].Pl=behindF[2];
       D->field[0].Sr=behindF[3];
       D->field[0].Sl=behindF[4];
    }
    else if(myrank%2==0 && myrank!=nTasks-1)
       MPI_Send(behindF,numberData, MPI_FLOAT, D->nextCore, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);

    //Transferring odd ~ even cores             
    behindF[0]=D->field[D->nx].E1;
    behindF[1]=D->field[D->nx].Pr;
    behindF[2]=D->field[D->nx].Pl;
    behindF[3]=D->field[D->nx].Sr;
    behindF[4]=D->field[D->nx].Sl;
        
    if(myrank%2==0 && myrank!=0)
    {
       MPI_Recv(behindF,numberData, MPI_FLOAT, D->beforeCore, D->beforeCore, MPI_COMM_WORLD,&status);  
       D->field[0].E1=behindF[0];
       D->field[0].Pr=behindF[1];
       D->field[0].Pl=behindF[2];
       D->field[0].Sr=behindF[3];
       D->field[0].Sl=behindF[4];
    }
    else if(myrank%2==1 && myrank!=nTasks-1)
       MPI_Send(behindF,numberData, MPI_FLOAT, D->nextCore, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);
    
    free(behindF);
}
*/

void MPI_TransferF_Yminus(Domain *D)
{
    int i,numberData,start,end;
    int myrank, nTasks; 
    float *btF;
    MPI_Status status;         
   
    MPI_Comm_size(MPI_COMM_WORLD, &nTasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);            

    numberData=6*D->nx;
    btF=(float *)malloc(numberData*sizeof(float ));             

    //Transferring even ~ odd cores 
    start=0; 
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].E1;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].B1;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Pr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Pl;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Sr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Sl;
        
    if(myrank%2==0 && myrank!=nTasks-1)
    {
       MPI_Recv(btF,numberData, MPI_FLOAT, myrank+1, myrank+1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].E1=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].B1=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Pr=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Pl=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Sr=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Sl=btF[i+start];
    }
    else if(myrank%2==1)
       MPI_Send(btF,numberData, MPI_FLOAT, myrank-1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);

    //Transferring odd ~ even cores             
    start=0; 
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].E1;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].B1;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Pr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Pl;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Sr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].Sl;
        
    if(myrank%2==1 && myrank!=nTasks-1)
    {
       MPI_Recv(btF,numberData, MPI_FLOAT, myrank+1, myrank+1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].E1=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].B1=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Pr=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Pl=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Sr=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].Sl=btF[i+start];
    }
    else if(myrank%2==0 && myrank!=0)
       MPI_Send(btF,numberData, MPI_FLOAT, myrank-1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);
    
    free(btF);
}

void MPI_TransferF_YminusC(Domain *D)
{
    int i,numberData,start,end;
    int myrank, nTasks; 
    float *btF;
    MPI_Status status;         
   
    MPI_Comm_size(MPI_COMM_WORLD, &nTasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);            

    numberData=2*D->nx;
    btF=(float *)malloc(numberData*sizeof(float ));             

    //Transferring even ~ odd cores 
    start=0; 
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].E1C;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].B1C;
        
    if(myrank%2==0 && myrank!=nTasks-1)
    {
       MPI_Recv(btF,numberData, MPI_FLOAT, myrank+1, myrank+1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].E1C=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].B1C=btF[i+start];
    }
    else if(myrank%2==1)
       MPI_Send(btF,numberData, MPI_FLOAT, myrank-1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);

    //Transferring odd ~ even cores             
    start=0; 
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].E1C;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       btF[start+i]=D->field[i+1][1].B1C;
        
    if(myrank%2==1 && myrank!=nTasks-1)
    {
       MPI_Recv(btF,numberData, MPI_FLOAT, myrank+1, myrank+1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].E1C=btF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][D->nySub+1].B1C=btF[i+start];
    }
    else if(myrank%2==0 && myrank!=0)
       MPI_Send(btF,numberData, MPI_FLOAT, myrank-1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);
    
    free(btF);
}

void MPI_TransferF_Yplus(Domain *D)
{
    int i,numberData,start,end;
    int myrank, nTasks; 
    float *upF;
    MPI_Status status;         
   
    MPI_Comm_size(MPI_COMM_WORLD, &nTasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);            

    numberData=4*D->nx;
    upF=(float *)malloc(numberData*sizeof(float ));             

    //Transferring even ~ odd cores 
    start=0; 
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Pr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Pl;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Sr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Sl;
    
    if(myrank%2==1)
    {
       MPI_Recv(upF,numberData, MPI_FLOAT, myrank-1, myrank-1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].Pr=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].Pl=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].Sr=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].Sl=upF[i+start];
    }
    else if(myrank%2==0 && myrank!=nTasks-1)
       MPI_Send(upF,numberData, MPI_FLOAT, myrank+1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);

    //Transferring odd ~ even cores             
    start=0; 
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Pr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Pl;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Sr;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].Sl;
        
    if(myrank%2==0 && myrank!=0)
    {
       MPI_Recv(upF,numberData, MPI_FLOAT, myrank-1, myrank-1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].Pr=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=start; i<end; i++)
          D->field[i+1][0].Pl=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=start; i<end; i++)
          D->field[i+1][0].Sr=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=start; i<end; i++)
          D->field[i+1][0].Sl=upF[i+start];
    }
    else if(myrank%2==1 && myrank!=nTasks-1)
       MPI_Send(upF,numberData, MPI_FLOAT, myrank+1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);
    
    free(upF);
}

void MPI_TransferF_YplusC(Domain *D)
{
    int i,numberData,start,end;
    int myrank, nTasks; 
    float *upF;
    MPI_Status status;         
   
    MPI_Comm_size(MPI_COMM_WORLD, &nTasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);            

    numberData=4*D->nx;
    upF=(float *)malloc(numberData*sizeof(float ));             

    //Transferring even ~ odd cores 
    start=0; 
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].PrC;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].PlC;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].SrC;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].SlC;
    
    if(myrank%2==1)
    {
       MPI_Recv(upF,numberData, MPI_FLOAT, myrank-1, myrank-1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].PrC=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].PlC=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].SrC=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].SlC=upF[i+start];
    }
    else if(myrank%2==0 && myrank!=nTasks-1)
       MPI_Send(upF,numberData, MPI_FLOAT, myrank+1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);

    //Transferring odd ~ even cores             
    start=0; 
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].PrC;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].PlC;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].SrC;
    start+=D->nx;
    for(i=0; i<D->nx; i++)
       upF[start+i]=D->field[i+1][D->nySub].SlC;
        
    if(myrank%2==0 && myrank!=0)
    {
       MPI_Recv(upF,numberData, MPI_FLOAT, myrank-1, myrank-1, MPI_COMM_WORLD,&status);  
       start=0;
       end=start+D->nx;
       for(i=0; i<D->nx; i++)
          D->field[i+1][0].PrC=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=start; i<end; i++)
          D->field[i+1][0].PlC=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=start; i<end; i++)
          D->field[i+1][0].SrC=upF[i+start];
       start=end;
       end=start+D->nx;
       for(i=start; i<end; i++)
          D->field[i+1][0].SlC=upF[i+start];
    }
    else if(myrank%2==1 && myrank!=nTasks-1)
       MPI_Send(upF,numberData, MPI_FLOAT, myrank+1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);
    
    free(upF);
}

/*
void MPI_TransferJ_Moving(Domain *D)
{
    int i,rank,numberData;
    int myrank, nTasks; 
    float *frontJ;
    MPI_Status status;         
   
    MPI_Comm_size(MPI_COMM_WORLD, &nTasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);            

    numberData=3;
    frontJ=(float *)malloc(numberData*sizeof(float ));             

    //Transferring even ~ odd cores             
    frontJ[0]=D->field[1].J1;
    frontJ[1]=D->field[1].J2;
    frontJ[2]=D->field[1].J3;
        
    if(myrank%2==0 && myrank!=nTasks-1)
    {
       MPI_Recv(frontJ,numberData, MPI_FLOAT, myrank+1, myrank+1, MPI_COMM_WORLD,&status);  
       D->field[D->nx+1].J1=frontJ[0];
       D->field[D->nx+1].J2=frontJ[1];
       D->field[D->nx+1].J3=frontJ[2];
    }
    else if(myrank%2==1)
       MPI_Send(frontJ,numberData, MPI_FLOAT, myrank-1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);

    //Transferring odd ~ even cores             
    frontJ[0]=D->field[1].J1;
    frontJ[1]=D->field[1].J2;
    frontJ[2]=D->field[1].J3;
        
    if(myrank%2==1 && myrank!=nTasks-1)
    {
       MPI_Recv(frontJ,numberData, MPI_FLOAT, myrank+1, myrank+1, MPI_COMM_WORLD,&status);  
       D->field[D->nx+1].J1=frontJ[0];
       D->field[D->nx+1].J2=frontJ[1];
       D->field[D->nx+1].J3=frontJ[2];
    }
    else if(myrank%2==0 && myrank!=0)
       MPI_Send(frontJ,numberData, MPI_FLOAT, myrank-1, myrank, MPI_COMM_WORLD);             
    MPI_Barrier(MPI_COMM_WORLD);
    
    free(frontJ);
}
*/
